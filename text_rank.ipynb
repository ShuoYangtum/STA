{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66f1ee62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "\n",
    "# Function to preprocess the text\n",
    "def preprocess_text(text):\n",
    "    # Tokenize text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Tokenize sentences into words and remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "    words = [[word for word in sentence if word not in stop_words] for sentence in words]\n",
    "\n",
    "    # Lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [[lemmatizer.lemmatize(word) for word in sentence] for sentence in words]\n",
    "\n",
    "    return words\n",
    "\n",
    "def get_cos_similar(v1: list, v2: list):\n",
    "    num = float(np.dot(v1, v2))  # 向量点乘\n",
    "    denom = np.linalg.norm(v1) * np.linalg.norm(v2)  # 求模长的乘积\n",
    "    return 0.5 + 0.5 * (num / denom) if denom != 0 else 0\n",
    "\n",
    "# Function to calculate sentence similarity using cosine similarity\n",
    "def sentence_similarity(sentence1, sentence2, sim_model='sbert', model=None):\n",
    "    # Convert sentences to sets of unique words\n",
    "    if sim_model=='sbert' and model:\n",
    "        sentence_embeddings = model.encode([sentence1, sentence2])\n",
    "        return float(get_cos_similar(sentence_embeddings[0], sentence_embeddings[1]))\n",
    "    else:\n",
    "        '''\n",
    "        $$ \\text{similarity}(s_1, s_2) = \\frac{\\sum_{w \\in s_1 \\cap s_2} c_1(w) \\times c_2(w)}{\\sqrt{\\sum_{w \\in s_1} c_1(w)^2} \\times \\sqrt{\\sum_{w \\in s_2} c_2(w)^2}} $$\n",
    "        '''\n",
    "        s1 = set(sentence1)\n",
    "        s2 = set(sentence2)\n",
    "\n",
    "        # Calculate the intersection of the two sets\n",
    "        intersection = s1.intersection(s2)\n",
    "\n",
    "        # Calculate the cosine similarity\n",
    "        numerator = sum([sentence1.count(word) * sentence2.count(word) for word in intersection])\n",
    "        denominator = math.sqrt(sum([sentence1.count(word)**2 for word in sentence1])) * math.sqrt(sum([sentence2.count(word)**2 for word in sentence2]))\n",
    "\n",
    "        if denominator == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return numerator / denominator\n",
    "\n",
    "# Function to calculate sentence scores using TextRank algorithm\n",
    "def textrank(sentences, d=0.85, max_iter=100, sim_model='sbert', model=None):\n",
    "    # Preprocess the text\n",
    "    if sim_model=='sbert' and model:\n",
    "        words=sentences.copy()\n",
    "    else:\n",
    "        words = preprocess_text(' '.join(sentences))\n",
    "\n",
    "    # Create a graph with sentences as nodes and edges between similar sentences\n",
    "    graph = {}\n",
    "    for i in range(len(words)):\n",
    "        for j in range(i+1, len(words)):\n",
    "            similarity = sentence_similarity(words[i], words[j], sim_model=sim_model, model=model)\n",
    "            if similarity > 0:\n",
    "                if i not in graph:\n",
    "                    graph[i] = {}\n",
    "                if j not in graph:\n",
    "                    graph[j] = {}\n",
    "                graph[i][j] = similarity\n",
    "                graph[j][i] = similarity\n",
    "\n",
    "    # Apply the TextRank algorithm\n",
    "    scores = [1.0] * len(words)\n",
    "    for i in range(max_iter):\n",
    "        new_scores = [0.0] * len(words)\n",
    "        for j in range(len(words)):\n",
    "            for k in graph[j]:\n",
    "                new_scores[k] += d * graph[j][k] * scores[j]\n",
    "            new_scores[j] += 1 - d\n",
    "        scores = new_scores\n",
    "\n",
    "    # Normalize the scores\n",
    "    max_score = max(scores)\n",
    "    if max_score > 0:\n",
    "        scores = [score / max_score for score in scores]\n",
    "\n",
    "    # Map scores to sentences\n",
    "    sentence_scores = {}\n",
    "    for i in range(len(sentences)):\n",
    "        sentence_scores[sentences[i]] = scores[i]\n",
    "\n",
    "    return sentence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea154804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(text):\n",
    "    if text.strip() == \"\":\n",
    "        return None\n",
    "    else:\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        if len(sentences) == 0:\n",
    "            return None\n",
    "        elif len(sentences) == 1:\n",
    "            return sentences[0]\n",
    "        else:\n",
    "            last_sentence = sentences[-1]\n",
    "            if last_sentence.strip() != text.strip():\n",
    "                return \" \".join(sentences[:-1])\n",
    "            else:\n",
    "                return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "95de1355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cat is on the mat.: 1.0\n",
      "The feline is resting on the carpet.: 0.9927521924046663\n",
      "The kitty is lounging on the rug.: 0.9864328292499707\n",
      "The puss is reclining on the doormat.: 0.9249253655469024\n",
      "The tomcat is lying on the floor covering.: 0.9285733841261431\n",
      "The man went to the store to buy some bread.: 0.8565111567116346\n",
      "The woman drove to the beach to go for a swim.: 0.8106649435816999\n",
      "The dog barked at the mailman when he delivered the mail.: 0.930270909468893\n",
      "The bird sang a sweet melody in the morning.: 0.8869145616308369\n",
      "The tree lost its leaves in the autumn breeze.: 0.8791913421101928\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "import random\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "sentences = [\n",
    "\"The cat is on the mat.\",\n",
    "\"The feline is resting on the carpet.\",\n",
    "\"The kitty is lounging on the rug.\",\n",
    "\"The puss is reclining on the doormat.\",\n",
    "\"The tomcat is lying on the floor covering.\",\n",
    "\"The man went to the store to buy some bread.\",\n",
    "\"The woman drove to the beach to go for a swim.\",\n",
    "\"The dog barked at the mailman when he delivered the mail.\",\n",
    "\"The bird sang a sweet melody in the morning.\",\n",
    "\"The tree lost its leaves in the autumn breeze.\"\n",
    "]\n",
    "#random.shuffle(sentences)\n",
    "scores = textrank(sentences, sim_model='sbert', model=model)\n",
    "\n",
    "for sentence, score in scores.items():\n",
    "    print(f\"{sentence}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b258a9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = preprocess_text(' '.join(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2b602514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I agree. I agree.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_sentences('I agree. I agree. I think the real disadvantages of the mobile phone are that, although it can also distract the mind and help others, it is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "159175bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.118813254"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a8967180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c19ed25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5f553245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the second sentence. This is the third sentence. This is the first sentence.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "dd35859f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The ス shaft golden # # movie stan.'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "9be8fd32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "dcd1fab1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843c1cde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc586a21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc0e64e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e804d55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
