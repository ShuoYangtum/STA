{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65f09ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- File reading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 237.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Model loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:03<00:00, 30.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Prepare for training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate`: Run `pip install --upgrade accelerate`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8220\\2519897306.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# 定义训练参数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     training_args = TrainingArguments(\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0moutput_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./results'\u001b[0m\u001b[1;33m,\u001b[0m      \u001b[1;31m# 输出目录\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0moverwrite_output_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\training_args.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, no_cuda, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, gradient_checkpointing, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, xpu_backend)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\training_args.py\u001b[0m in \u001b[0;36m__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1331\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"pt\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1332\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1333\u001b[1;33m             \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"cuda\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1334\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mget_xla_device_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"GPU\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1335\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp16\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp16_full_eval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\training_args.py\u001b[0m in \u001b[0;36mdevice\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1695\u001b[0m         \"\"\"\n\u001b[0;32m   1696\u001b[0m         \u001b[0mrequires_backends\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"torch\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1697\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setup_devices\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1699\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\utils\\generic.py\u001b[0m in \u001b[0;36m__get__\u001b[1;34m(self, obj, objtype)\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mcached\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcached\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m             \u001b[0mcached\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m             \u001b[0msetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcached\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\training_args.py\u001b[0m in \u001b[0;36m_setup_devices\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1611\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"PyTorch: setting up devices\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1612\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_sagemaker_mp_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_accelerate_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheck_partial_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1613\u001b[1;33m             raise ImportError(\n\u001b[0m\u001b[0;32m   1614\u001b[0m                 \u001b[1;34m\"Using the `Trainer` with `PyTorch` requires `accelerate`: Run `pip install --upgrade accelerate`\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1615\u001b[0m             )\n",
      "\u001b[1;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate`: Run `pip install --upgrade accelerate`"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "BATCH_SIZE=16\n",
    "LR=0.0001\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 构建DataCollator对象\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "# 读取自己的语料库，一行代表一个样本\n",
    "print('-- File reading...')\n",
    "# 构建TextDataset对象\n",
    "with tqdm(total=100) as pbar:\n",
    "    train_dataset = TextDataset(tokenizer=tokenizer, file_path='./dataset/dailydialog/train.txt', block_size=128)\n",
    "    pbar.update(90)\n",
    "    eval_dataset = TextDataset(tokenizer=tokenizer, file_path='./dataset/dailydialog/test.txt', block_size=128)\n",
    "    pbar.update(10)\n",
    "print('-- Model loading...')\n",
    "# 读取GPT2预训练模型\n",
    "with tqdm(total=100) as pbar:\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "    pbar.update(100)\n",
    "print('-- Prepare for training...')\n",
    "# 定义训练参数\n",
    "with tqdm(total=100) as pbar:\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',      # 输出目录\n",
    "        overwrite_output_dir=False,\n",
    "        num_train_epochs=1,          # 训练轮数\n",
    "        per_device_train_batch_size=BATCH_SIZE,  # 每个设备的训练批次大小\n",
    "        save_total_limit=10,             # 最多保存的模型数量\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        evaluation_strategy='epoch',\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=2,\n",
    "        learning_rate=LR,\n",
    "        weight_decay=0.005,\n",
    "        warmup_steps=100,\n",
    "        logging_steps=200,\n",
    "        save_steps=2000,\n",
    "    )\n",
    "    # 初始化Trainer对象\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    pbar.update(100)\n",
    "print('-- Start training...')\n",
    "# 开始训练\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96b8ef1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1279,    66,    29, 13816,   837,  5395,   837,   703,   546,  1016,\n",
       "          329,   257,  1178, 16800,   706,  8073,  5633,  1279,    80,    29,\n",
       "          921,   760,   326,   318, 29850,   475,   318,  1107,   407,   922,\n",
       "          329,   674, 13547,   764,  1279,    64,    29,   198, 25515,   837,\n",
       "         5395,   837,   703,   546,  1016,   329,   257,  1178, 16800,   706,\n",
       "         8073,  5633,  1279,    66,    29,   921,   760,   326,   318, 29850,\n",
       "          475,   318,  1107,   407,   922,   329,   674, 13547,   764,  1279,\n",
       "           80,    29,  1867,   466,   345,  1612,  5633,   632,   481,  1037,\n",
       "          514,   284,  8960,   764,  1279,    64,    29,   198, 25515,   837,\n",
       "         5395,   837,   703,   546,  1016,   329,   257,  1178, 16800,   706,\n",
       "         8073,  5633,   921,   760,   326,   318, 29850,   475,   318,  1107,\n",
       "          407,   922,   329,   674, 13547,   764,  1279,    66,    29,  1867,\n",
       "          466,   345,  1612,  5633,   632,   481,  1037,   514])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea02882",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
